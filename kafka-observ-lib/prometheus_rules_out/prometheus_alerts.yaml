groups:
    - name: kafka-kafka-alerts
      rules:
        - alert: KafkaLagKeepsIncreasing
          annotations:
            description: 'Kafka lag keeps increasing over the last 15 minutes for consumer group: {{$labels.consumergroup}}, topic: {{$labels.topic}}.'
            summary: Kafka lag keeps increasing.
          expr: |-
            sum by (kafka_cluster, topic, consumergroup) (delta(kafka_consumergroup_lag{topic!="__consumer_offsets",consumergroup!="",}[5m])
            or
            delta(kafka_consumergroup_uncommitted_offsets{topic!="__consumer_offsets",consumergroup!="",}[5m])) > 0
          for: 15m
          keep_firing_for: 10m
          labels:
            severity: warning
        - alert: KafkaLagIsTooHigh
          annotations:
            description: 'Total kafka lag across all partitions is too high ({{ printf "%.0f" $value }}) for consumer group: {{$labels.consumergroup}}, topic: {{$labels.topic}}.'
            summary: Kafka lag is too high.
          expr: |-
            sum by (kafka_cluster, topic, consumergroup) (kafka_consumergroup_lag{topic!="__consumer_offsets",consumergroup!="",}
            or
            kafka_consumergroup_uncommitted_offsets{topic!="__consumer_offsets",consumergroup!="",}) > 100
          for: 15m
          keep_firing_for: 5m
          labels:
            severity: critical
        - alert: KafkaISRExpandRate
          annotations:
            description: |
                Kafka broker {{ $labels.instance }} in cluster {{ $labels.kafka_cluster }} has In-Sync Replica (ISR) expanding at {{ printf "%.2f" $value }} per second.

                ISR expansion typically occurs when a broker recovers and its replicas catch up to the leader. The expected steady-state value for ISR expansion rate is 0.

                Frequent ISR expansion and shrinkage indicates instability and may suggest:
                - Brokers frequently going offline/online
                - Network connectivity issues
                - Replica lag configuration too tight (adjust replica.lag.max.messages or replica.socket.timeout.ms)
                - Insufficient broker resources causing replicas to fall behind

                If this alert fires frequently without corresponding broker outages, investigate broker health and adjust replica lag settings.
            summary: Kafka ISR expansion detected.
          expr: |
            sum by (kafka_cluster,instance) (sum by (kafka_cluster,instance) (kafka_server_replicamanager_isrexpandspersec{})
            or
            sum by (kafka_cluster,instance) (rate(kafka_server_replicamanager_isrexpands_total{}[5m]))
            or
            sum by (kafka_cluster,instance) (rate(kafka_server_replicamanager_total_isrexpandspersec_count{}[5m]))) != 0
          for: 5m
          keep_firing_for: 15m
          labels:
            severity: warning
        - alert: KafkaISRShrinkRate
          annotations:
            description: |
                Kafka broker {{ $labels.instance }} in cluster {{ $labels.kafka_cluster }} has In-Sync Replica (ISR) shrinking at {{ printf "%.2f" $value }} per second.

                ISR shrinkage occurs when a replica falls too far behind the leader and is removed from the ISR set. This reduces fault tolerance as fewer replicas are in-sync.
                The expected steady-state value for ISR shrink rate is 0.

                Common causes include:
                - Broker failures or restarts
                - Network latency or connectivity issues
                - Replica lag exceeding replica.lag.max.messages threshold
                - Replica not contacting leader within replica.socket.timeout.ms
                - Insufficient broker resources (CPU, disk I/O, memory)
                - High producer throughput overwhelming broker capacity

                If ISR is shrinking without corresponding expansion shortly after, investigate broker health, network connectivity, and resource utilization.
                Consider adjusting replica.lag.max.messages or replica.socket.timeout.ms if shrinkage is frequent but brokers are healthy.
            summary: Kafka ISR shrinkage detected.
          expr: |
            sum by (kafka_cluster,instance) (sum by (kafka_cluster,instance)  (rate(kafka_server_replicamanager_isrshrinks_total{}[5m]))
            or
            sum by (kafka_cluster,instance)  (rate(kafka_server_replicamanager_total_isrshrinkspersec_count{}[5m]))
            or
            sum by (kafka_cluster,instance) (kafka_server_replicamanager_isrshrinkspersec{})) != 0
          for: 5m
          keep_firing_for: 15m
          labels:
            severity: warning
        - alert: KafkaOfflinePartitionCount
          annotations:
            description: |
                Kafka cluster {{ $labels.kafka_cluster }} has {{ printf "%.0f" $value }} offline partitions.

                Offline partitions have no active leader, making them completely unavailable for both reads and writes. This directly impacts application functionality.

                Common causes include:
                - All replicas for the partition are down
                - No in-sync replicas available for leader election
                - Cluster controller issues preventing leader election
                - Insufficient replica count for the replication factor

                Immediate actions:
                1. Check broker status - identify which brokers are down
                2. Review broker logs for errors and exceptions
                3. Restart failed brokers if needed
                4. Verify ZooKeeper connectivity
                5. Check for disk space or I/O issues on broker hosts
                6. Monitor ISR status to ensure replicas are catching up

                Until resolved, affected topics cannot serve traffic for these partitions.
            summary: Kafka has offline partitions.
          expr: |
            sum by (kafka_cluster) (kafka_controller_kafkacontroller_offlinepartitionscount_value{}
            or
            kafka_controller_kafkacontroller_offlinepartitionscount{}) > 0
          for: 5m
          keep_firing_for: 5m
          labels:
            severity: critical
        - alert: KafkaUnderReplicatedPartitionCount
          annotations:
            description: |
                Kafka broker {{ $labels.instance }} in cluster {{ $labels.kafka_cluster }} has {{ printf "%.0f" $value }} under-replicated partitions.

                Under-replicated partitions have fewer in-sync replicas (ISR) than the configured replication factor, reducing fault tolerance and risking data loss.

                Impact:
                - Reduced data durability (fewer backup copies)
                - Increased risk of data loss if additional brokers fail
                - Lower fault tolerance for partition availability

                Common causes:
                - Broker failures or network connectivity issues
                - Brokers unable to keep up with replication (resource constraints)
                - High producer throughput overwhelming replica capacity
                - Disk I/O saturation on replica brokers
                - Network partition between brokers

                Actions:
                1. Identify which brokers are lagging (check ISR for affected partitions)
                2. Review broker resource utilization (CPU, memory, disk I/O)
                3. Check network connectivity between brokers
                4. Verify broker logs for replication errors
                5. Consider adding broker capacity if consistently under-replicated
                6. Reassign partitions if specific brokers are problematic
            summary: Kafka has under-replicated partitions.
          expr: |
            sum by (kafka_cluster,instance) (kafka_cluster_partition_underreplicated{}) > 0
          for: 5m
          keep_firing_for: 5m
          labels:
            severity: critical
        - alert: KafkaUnderMinISRPartitionCount
          annotations:
            description: |
                Kafka cluster {{ $labels.kafka_cluster }} has {{ printf "%.0f" $value }} partitions with fewer in-sync replicas than min.insync.replicas configuration.

                CRITICAL IMPACT: These partitions are UNAVAILABLE FOR WRITES when producers use acks=all, directly impacting application availability.

                This configuration prevents data loss by refusing writes when not enough replicas are in-sync, but at the cost of availability.

                Common causes:
                - Broker failures reducing available replicas below threshold
                - Network issues preventing replicas from staying in-sync
                - Brokers overwhelmed and unable to keep up with replication
                - Recent partition reassignment or broker maintenance

                Immediate actions:
                1. Identify affected partitions and their current ISR status
                2. Check broker health and availability
                3. Review network connectivity between brokers
                4. Investigate broker resource utilization (CPU, disk I/O, memory)
                5. Restart failed brokers or resolve broker issues
                6. Monitor ISR recovery as brokers catch up

                Producers will receive NOT_ENOUGH_REPLICAS errors until ISR count recovers above min.insync.replicas threshold.
            summary: Kafka partitions below minimum ISR - writes unavailable.
          expr: |
            sum by (kafka_cluster) (kafka_cluster_partition_underminisr{}) > 0
          for: 2m
          keep_firing_for: 5m
          labels:
            severity: critical
        - alert: KafkaPreferredReplicaImbalance
          annotations:
            description: |
                Kafka cluster {{ $labels.kafka_cluster }} has {{ $value }} partitions where the leader is not the preferred replica.

                Impact:
                Uneven load distribution across brokers can result in some brokers handling significantly more client requests (produce/consume) than others, leading to hotspots, degraded performance, and potential resource exhaustion on overloaded brokers. This prevents optimal cluster utilization and can impact latency and throughput.

                Common causes:
                - Broker restarts or failures causing leadership to shift to non-preferred replicas
                - Manual partition reassignments or replica movements
                - Recent broker additions to the cluster
                - Failed automatic preferred replica election
                - Auto leader rebalancing disabled (auto.leader.rebalance.enable=false)

                Actions:
                1. Verify auto.leader.rebalance.enable is set to true in broker configuration
                2. Check leader.imbalance.check.interval.seconds (default 300s) configuration
                3. Manually trigger preferred replica election using kafka-preferred-replica-election tool
                4. Monitor broker resource utilization (CPU, network) for imbalance
                5. Review broker logs for leadership election errors
                6. Verify all brokers are healthy and reachable

                If the imbalance persists for extended periods, consider running manual preferred replica election to redistribute leadership and restore balanced load across the cluster.
            summary: Kafka has preferred replica imbalance.
          expr: |
            sum by (kafka_cluster) (kafka_controller_kafkacontroller_preferredreplicaimbalancecount_value{}
            or
            kafka_controller_kafkacontroller_preferredreplicaimbalancecount{}) > 0
          for: 30m
          keep_firing_for: 5m
          labels:
            severity: warning
        - alert: KafkaNoActiveController
          annotations:
            description: |
                Kafka cluster {{ $labels.kafka_cluster }} has {{ $value }} broker(s) reporting as the active controller. Expected exactly 1 active controller.

                CRITICAL impact:
                The Kafka controller is responsible for cluster-wide administrative operations including partition leader election, broker failure detection, topic creation/deletion, and partition reassignment. Without an active controller (value=0) or with multiple controllers (value>1), the cluster cannot perform these critical operations, potentially causing:
                - Inability to elect new partition leaders when brokers fail
                - Topic creation/deletion operations hang indefinitely
                - Partition reassignments cannot be executed
                - Cluster metadata inconsistencies
                - Split-brain scenarios if multiple controllers exist

                Common causes:
                - Zookeeper connectivity issues or Zookeeper cluster instability
                - Network partitions between brokers and Zookeeper
                - Controller broker crash or unclean shutdown
                - Long garbage collection pauses on controller broker
                - Zookeeper session timeout (zookeeper.session.timeout.ms exceeded)
                - Controller election conflicts during network splits

                This is a critical cluster-wide issue requiring immediate attention to restore normal operations.
            summary: Kafka has no active controller.
          expr: |-
            sum by(kafka_cluster) (kafka_controller_kafkacontroller_activecontrollercount_value{}
            or
            kafka_controller_kafkacontroller_activecontrollercount{}) != 1
          for: 5m
          labels:
            severity: critical
        - alert: KafkaUncleanLeaderElection
          annotations:
            description: |
                Kafka cluster {{ $labels.kafka_cluster }} has {{ $value }} unclean partition leader elections reported in the last 5 minutes.

                CRITICAL Impact - DATA LOSS RISK:
                Unclean leader election occurs when no in-sync replica (ISR) is available to become the leader, forcing Kafka to elect an out-of-sync replica. This WILL result in data loss for any messages that were committed to the previous leader but not replicated to the new leader. This compromises data durability guarantees and can cause:
                - Permanent loss of committed messages
                - Consumer offset inconsistencies
                - Duplicate message processing
                - Data inconsistencies between producers and consumers
                - Violation of at-least-once or exactly-once semantics

                Common causes:
                - All ISR replicas failed simultaneously (broker crashes, hardware failures)
                - Network partitions isolating all ISR members
                - Extended broker downtime exceeding replica lag tolerance
                - Insufficient replication factor (RF < 3) combined with broker failures
                - min.insync.replicas set too low relative to replication factor
                - Disk failures on multiple replicas simultaneously
                - Aggressive unclean.leader.election.enable=true configuration

                Immediate actions:
                1. Review broker logs to identify which partitions had unclean elections
                2. Investigate root cause of ISR replica failures (check broker health, hardware, network)
                3. Assess data loss impact by comparing producer and consumer offsets for affected partitions
                4. Alert application teams to potential data loss in affected partitions
                5. Bring failed ISR replicas back online as quickly as possible
                6. Consider resetting consumer offsets if data loss is unacceptable
                7. Review and increase replication factor for critical topics (minimum RF=3)
                8. Set unclean.leader.election.enable=false to prevent future unclean elections (availability vs. durability trade-off)
                9. Increase min.insync.replicas to strengthen durability guarantees
                10. Implement better monitoring for ISR shrinkage to detect issues before unclean elections occur

                This indicates a serious reliability event that requires immediate investigation and remediation to prevent future data loss.
            summary: Kafka has unclean leader elections.
          expr: |-
            (sum by (kafka_cluster,instance) (kafka_controller_controllerstats_uncleanleaderelectionspersec{})
            or
            sum by (kafka_cluster,instance) (rate(kafka_controller_controllerstats_uncleanleaderelections_total{}[5m]))) != 0
          for: 5m
          keep_firing_for: 5m
          labels:
            severity: critical
        - alert: KafkaBrokerCount
          annotations:
            description: |+
                Kafka cluster {{ $labels.kafka_cluster }} has zero brokers reporting metrics.

                No brokers are online or reporting metrics, indicating complete cluster failure. This results in:
                - Total unavailability of all topics and partitions
                - All produce and consume operations failing
                - Complete loss of cluster functionality
                - Potential data loss if unclean shutdown occurred
                - Application downtime for all services depending on Kafka

            summary: Kafka has no brokers online.
          expr: |-
            count by(kafka_cluster) (kafka_server_kafkaserver_brokerstate{}
            or
            kafka_server_kafkaserver_total_brokerstate_value{}) == 0
          for: 5m
          labels:
            severity: critical
        - alert: KafkaZookeeperSyncConnect
          annotations:
            description: |
                Kafka broker {{ $labels.instance }} in cluster {{ $labels.kafka_cluster }} has lost connection to Zookeeper.

                Zookeeper connectivity is essential for Kafka broker operation. A disconnected broker cannot:
                - Participate in controller elections
                - Register or maintain its broker metadata
                - Receive cluster state updates
                - Serve as partition leader (will be removed from ISR)
                - Handle leadership changes or partition reassignments

                This will cause the broker to become isolated from the cluster, leading to under-replicated partitions and potential service degradation for any topics hosted on this broker.

                Prolonged Zookeeper disconnection will result in the broker being ejected from the cluster and leadership reassignments.
            summary: Kafka Zookeeper sync disconnected.
          expr: |-
            avg by(kafka_cluster,instance) (rate(kafka_server_sessionexpirelistener_zookeepersyncconnects_total{quantile="0.95",}[5m])
            or
            rate(kafka_server_sessionexpirelistener_zookeepersyncconnectspersec{quantile="0.95",}[5m])) < 0
          for: 5m
          labels:
            severity: critical
    - name: kafka-jvm-alerts
      rules:
        - alert: JvmMemoryFillingUp
          annotations:
            description: JVM heap memory usage is at {{ printf "%.0f" $value }}% over the last 5 minutes on {{$labels.instance}}, which is above the threshold of 80%.
            summary: JVM heap memory filling up.
          expr: |-
            ((java_lang_memory_heapmemoryusage_used{}
            or
            sum without (id) (jvm_memory_bytes_used{area="heap", })
            or
            sum without (id) (jvm_memory_used_bytes{area="heap", }))/(java_lang_memory_heapmemoryusage_max{} != -1
            or
            sum without (id) (jvm_memory_bytes_max{area="heap", } != -1)
            or
            sum without (id) (jvm_memory_max_bytes{area="heap", } != -1))) * 100 > 80
          for: 5m
          keep_firing_for: 5m
          labels:
            severity: warning
        - alert: JvmThreadsDeadlocked
          annotations:
            description: 'JVM deadlock detected: Threads in the JVM application {{$labels.instance}} are in a cyclic dependency with each other. The restart is required to resolve the deadlock.'
            summary: JVM deadlock detected.
          expr: (jvm_threads_deadlocked{}) > 0
          for: 2m
          keep_firing_for: 5m
          labels:
            severity: critical
    - name: kafka-zookeeper-jvm-alerts
      rules:
        - alert: JvmMemoryFillingUp
          annotations:
            description: JVM heap memory usage is at {{ printf "%.0f" $value }}% over the last 5 minutes on {{$labels.instance}}, which is above the threshold of 80%.
            summary: JVM heap memory filling up.
          expr: ((sum without (id) (jvm_memory_bytes_used{area="heap", }))/(sum without (id) (jvm_memory_bytes_max{area="heap", } != -1))) * 100 > 80
          for: 5m
          keep_firing_for: 5m
          labels:
            severity: warning
        - alert: JvmThreadsDeadlocked
          annotations:
            description: 'JVM deadlock detected: Threads in the JVM application {{$labels.instance}} are in a cyclic dependency with each other. The restart is required to resolve the deadlock.'
            summary: JVM deadlock detected.
          expr: (jvm_threads_deadlocked{}) > 0
          for: 2m
          keep_firing_for: 5m
          labels:
            severity: critical
